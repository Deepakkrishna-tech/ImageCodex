ğŸ¬ ImageCodeX

ImageCodeX is an advanced agentic, multi-agent-powered visual engineering tool designed to transform static images and text prompts into a complete creative workflow. By combining cutting-edge artificial intelligence with intuitive human-in-the-loop collaboration, ImageCodeX empowers users to craft compelling narratives, dynamic video prompts, and high-quality AI-generated images.

![alt text](https://img.shields.io/badge/Python-3.10+-blue.svg)


![alt text](https://img.shields.io/badge/Framework-Streamlit-FF4B4B)


![alt text](https://img.shields.io/badge/Agents-LangGraph-E86F2C)


![alt text](https://img.shields.io/badge/License-MIT-green.svg)

At its core, ImageCodeX leverages a sophisticated multi-stage workflow that guides users through every step of the creative process:

Stage 1 & 2: Visual Prompt Engineering â€“ Analyze static images to generate and refine detailed, model-ready text prompts for both image and cinematic video generation.

Stage 3: Narrative Brainstorming â€“ Expand a visual or text-based idea into multiple high-level story concepts, complete with loglines and director-style pitches.

Stage 4: AI Image Generation â€“ Close the creative loop by generating high-quality images directly from your engineered prompts using a variety of state-of-the-art models.

With its seamless integration of AI-driven insights and human creativity, ImageCodeX bridges the gap between imagination and executionâ€”making it the ultimate tool for storytellers, content creators, and AI enthusiasts.

âœ¨ Features

Stage 1 & 2: Visual Prompting ğŸ–¼ï¸ ğŸ¥

Upload an image to receive a detailed, model-ready text-to-image prompt.

Automated analysis of artistic elements, style, mood, and composition.

Extend prompts with creative briefs (mood, camera movement) for cinematic video concepts.

Prompt critique and refinement with user feedback.

Stage 3: Narrative Engine ğŸ“–

Brainstorm multiple distinct story concepts from a single image or text idea.

Generate unique titles, loglines, synopses, and director styles for each concept.

Supports genre and mood customization.

Stage 4: AI Image Generation ğŸ¨

Multi-Model Support: Generate images using GPT-4o, Stable Diffusion XL, and Kandinsky 2.2.

Text-to-Image: Create visuals directly from your crafted prompts.

Image-to-Image & Variations: Provide an optional reference image to guide generation or create variations.

Download Functionality: Easily save your generated images with a single click.

Developer Tools ğŸ”§

Dev sidebar for inspecting and clearing the application's real-time state.

Modular, agent-based architecture built with LangGraph for easy extension.

ğŸ¥ Demo

![alt text](docs/screenshot.png)

<!-- TODO: Replace with an updated screenshot showing the Stage 4 tab with a generated image. -->


![alt text](https://img.youtube.com/vi/your_video_id/0.jpg)

<!-- TODO: Replace with your actual YouTube video link -->

ğŸš€ Usage

Visual Prompting Tab (Stages 1 & 2)

Image Prompt: Upload an image and click "Analyze and Generate Prompt".

Video Prompt: Provide a creative brief and click "Generate Video Prompt" using either the first image or a new one.

Narrative Engine Tab (Stage 3)

Provide an image, text idea, genre, and mood.

Click "Brainstorm Concepts" to receive multiple story ideas.

Image Generation Tab (Stage 4) ğŸ¨

The prompt from Stage 1 is pre-filled, or you can write a new one.

Select your desired AI model (e.g., GPT-4o) and aspect ratio.

(Optional) Tick "Use Reference Image" and upload an image for img2img/variation.

Click "Generate Image" and review the result.

Click "Download Image" to save your creation.

ğŸ’» Tech Stack

Python 3.10+ ğŸ

Streamlit â€“ Interactive web UI ğŸŒŸ

Pydantic â€“ Data validation and modeling ğŸ“Š

LangChain / LangGraph â€“ Agentic workflow orchestration ğŸ¤–

OpenAI & Replicate APIs â€“ Powering LLM and image generation agents ğŸ§ 

Pillow â€“ Image processing ğŸ–¼ï¸

Requests - HTTP requests for downloading generated images

Poetry â€“ Dependency management ğŸ“¦

dotenv â€“ Environment variable management ğŸ”‘

ğŸ¤– Agentic Workflow & Agent Roles

The core of ImageCodeX is a multi-agent workflow orchestrated by LangGraph. Each agent is an expert in a specific creative or analytical task:

Agent Name	Role in Workflow
visual_analyst	Analyzes uploaded images for subject, style, mood, and composition.
prompt_engineer	Generates detailed text-to-image prompts based on the visual analysis.
inspector	Critiques the generated prompt for accuracy, clarity, and creative fit.
refiner	Refines prompts based on user feedback and inspector critique.
video_director	Converts image prompts and creative briefs into cinematic video prompts.
film_story_writer	Brainstorms multiple high-level story concepts from visual and textual input.
image_generator	Generates images using various models (GPT-4o, SDXL) via text-to-image or image-to-image.
script_expert	(Future) Expands story concepts into screenplay-style scenes and dialogue.
storyboard_artist	(Future) Generates visual storyboard descriptions for each scene.
ğŸ”„ How the Agentic Workflow Works

Visual Prompting (Stages 1 & 2)

visual_analyst analyzes the uploaded image.

prompt_engineer creates a text prompt.

inspector critiques the prompt.

refiner improves the prompt if user feedback is provided.

video_director uses the prompt and a creative brief to generate a video concept.

Narrative Engine (Stage 3)

film_story_writer builds multiple story concepts from the visual and user input.

Image Generation (Stage 4)

image_generator uses the final prompt (and optional reference image) to call the selected AI model (OpenAI or Replicate).

All agent steps are orchestrated as independent graphs (see src/graph/graphs.py), allowing for flexible, modular, and extensible workflows.

ğŸ“‚ File Structure
Generated code
ImageCodeX/
â”‚
â”œâ”€â”€ .env                  # API keys and environment variables ğŸ”‘
â”œâ”€â”€ pyproject.toml        # Poetry project config ğŸ“¦
â”œâ”€â”€ run_app.py            # Main application entry point â–¶ï¸
â”‚
â””â”€â”€ src/
    â”‚
    â”œâ”€â”€ app.py            # Main Streamlit app and controller ğŸŒŸ
    â”‚
    â”œâ”€â”€ agents/
    â”‚   â”œâ”€â”€ film_story_writer.py
    â”‚   â”œâ”€â”€ image_generator.py      # <-- ADDED
    â”‚   â”œâ”€â”€ inspector.py
    â”‚   â”œâ”€â”€ prompt_engineer.py
    â”‚   â”œâ”€â”€ refiner.py
    â”‚   â”œâ”€â”€ script_expert.py
    â”‚   â”œâ”€â”€ storyboard_artist.py
    â”‚   â”œâ”€â”€ video_director.py
    â”‚   â””â”€â”€ visual_analyst.py
    â”‚
    â”œâ”€â”€ core/
    â”‚   â”œâ”€â”€ prompts.py    # Prompt templates for agents ğŸ“
    â”‚   â””â”€â”€ schemas.py    # Pydantic models for app state and data ğŸ“Š
    â”‚
    â”œâ”€â”€ graph/
    â”‚   â””â”€â”€ graphs.py     # Workflow graphs connecting agents ğŸ”„
    â”‚
    â””â”€â”€ ui/
        â”œâ”€â”€ stage3_ui.py            # UI for narrative engine ğŸ“–
        â”œâ”€â”€ stage4_ui.py            # UI for image generation ğŸ¨  <-- ADDED
        â””â”€â”€ visual_prompting_ui.py  # UI for image/video prompt stages ğŸ–¼ï¸ğŸ¥

ğŸ› ï¸ Installation

Requirements:

Python 3.10+ ğŸ

Poetry ğŸ“¦

Git ğŸŒ¿

1. Clone the repository:

Generated sh
git clone https://github.com/Deepakkrishna-tech/ImageCodeX.git
cd ImageCodeX
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Sh
IGNORE_WHEN_COPYING_END

2. Create the Environment File:
Create a file named .env in the root of the project directory and add your API keys:

Generated env
OPENAI_API_KEY="sk-..."
REPLICATE_API_TOKEN="r8_..."
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Env
IGNORE_WHEN_COPYING_END

3. Install Dependencies:
This command will create a virtual environment and install all necessary packages.

Generated sh
poetry install
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Sh
IGNORE_WHEN_COPYING_END

4. Run the App:
Use the unified run_app.py script. This works on Windows, macOS, and Linux.

Generated sh
poetry run streamlit run run_app.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Sh
IGNORE_WHEN_COPYING_END
ğŸŒ± Extending & Contributing

Fork and clone the repo.

Add new agents or UI stages as needed.

Submit pull requests for improvements or bug fixes.

ğŸ“œ License

MIT License. See LICENSE for details. ğŸ“„